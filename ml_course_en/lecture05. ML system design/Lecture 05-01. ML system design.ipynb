{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lecture 05. ML system design\n",
    "## Error Metrics for Skewed Classes\n",
    "**What are Skewed Classes?**\n",
    "Skewed classes basically refer to a dataset, wherein the number of training example belonging to one class out-numbers heavily the number of training examples beloning to the other.\n",
    "\n",
    "Consider a binary classification, where a cancerous patient is to be detected based on some features. And say only 1  of the data provided has cancer positive. In a setting where having cancer is labelled 1 and not cancer labelled 0, if a system naively gives the prediction as all 0’s, still the prediction accuracy will be 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive prediction ignoring features\n",
    "def predict_cancer(x):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it can be said with conviction that the accuracy metrics or mean-squared error for skewed classes, is not a proper indicator of model performance. Hence, there is a need for a different error metric for skewed classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/Recall\n",
    "Note: $y = 1$ is ther rarer class among the two.\n",
    "\n",
    "In a binary classification, one of the following four scenarios may occur:\n",
    "\n",
    "* True Positive (TP): the model predicts 1 and the actual class is 1\n",
    "* True Negative (TN): the model predicts 0 and the actual class is 0\n",
    "* False Positive (FP): the model predicts 1 but the actual class is 0\n",
    "* False Negative (FN): the model predicts 0 but the actual class is 1\n",
    "\n",
    "![](../../img/lec05_f1.png)\n",
    "\n",
    "Then precision and recall can be defined as follows:\n",
    "\n",
    "* $precision = \\frac{TP}{(TP + FP)}$\n",
    "* $recall = \\frac{TP}{(TP + FN)}$\n",
    "\n",
    "Recall defines of all the actual $y = 1$, which ones did the model predict correctly.\n",
    "\n",
    "Now, if we evaluate a scenario where the classifier predicts all 0’s then the recall of the model will be 0, which then points out the inability of the system.\n",
    "\n",
    "**In case of skewed classes, it’s not possible for the classifiers to cheat the evaluation metrics of recall and precision. Also, it is important to note that precision and recall metrics work better if $y=1$, denotes the presence of the rarer class.**\n",
    "\n",
    "By changing the threshold value for the classifier confidence, one can adjust the precision and recall for the model.\n",
    "\n",
    "For example, in a logistic regression the threshold is generally at 0.5. If one increases it, we can be sure that of all the predictions made more will be correct, hence, high precision. But there are also higher chances of missing the positive cases, hence, the lower recall.\n",
    "\n",
    "Similary, if one decreases the threshold, then the chances of false positives increases, hence low precision. Also, there is lesser probability of missing the actual cases, hence high recall.\n",
    "\n",
    "A precision-recall tradeoff curve may look like one among the following,\n",
    "\n",
    "![](../../img/lec05_threshold.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $F_1$ score\n",
    "Given two pairs of precision and recall, how to choose the better pair. One of the options would be to choose the one which higher average. That is not the ideal solution as the pair with ($precision=0.02$ and $recall = 1$) has a better mean than the pair ($precision=0.5$ and $recall=0.4$).\n",
    "\n",
    "Enter $F score$ or $F_1 score$, which is the harmonic mean of precision and recall, defined as\n",
    "\n",
    " $F_1 = \\frac{2 P*R}{P+R}$\n",
    " \n",
    " The above formula has advantage over the average method because, if either precision or recall is small, the the numerator product $P∗R$ will weigh the $F_1 - score$ low and consequently lead to choosing the better pair of precision and recall. So,\n",
    " * if $P=0$ or $R=0$, then $F_1 = 0$\n",
    " * if $P=1$ and $R=1$, then $F_1 = 1$\n",
    " \n",
    " **One reasonable way of automatically choosing threshold for classifier is to try a range of them on the cross-validation set and pick the one that gives the highest F-Score.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity/Specifivity\n",
    "Apart from precision and recall, sensitivity and specifivity are among the most used error metrics in classfication.\n",
    "\n",
    "* Sensitivity or True Positive Rate (TPR) is another name for recall and is also called hit rate \n",
    "$TPR=\\frac{TP}{TP+FN}$\n",
    "* Specifivity (SPC) or True Negative Rate \n",
    "$SPC=\\frac{TN}{TN+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning diagnostic\n",
    "A test that you can run to gain insight what isn't working witha  learning algorythm, and gain guidance as to how best to improve its performance.\n",
    "\n",
    "Diagnostic can take time to implement, but doing so can be a very good use of your time.\n",
    "\n",
    "### Diagnosing bias vs variance\n",
    "In Machine Learning, suppose our learning algorithm (e.g.Linear Regression or Random Forest etc.) makes huge errors while predicting data. So what can we do to improve our learning algorithm? We can reduce these errors by trying something like:\n",
    "\n",
    "* Getting more training examples\n",
    "* Trying smaller sets of features\n",
    "* Trying additional features\n",
    "* Trying polynomial features\n",
    "* Increasing or decreasing Regularization Parameters\n",
    "\n",
    "But to try out all or any of these options we might end up spending a lot of time to understand what is making our algorithm perform poorly.\n",
    "\n",
    "In other words if you run a Learning Algorithm and it didn't do well then almost all the time its either a high Bias problem or high Variance problem. These are the two terms in statistics. So we need to distinguish whether bias or variance is the problem contributing to bad predictions by evaluating a hypothesis that has been leant by our learning algorithm to understand if its overfitting or under-fitting?\n",
    "\n",
    "* The Bias measures how far off in general these models’ predictions are from the correct value.\n",
    "* The variance is how much the predictions for a given point vary between different realizations of the model.\n",
    "* High bias is under-fitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.\n",
    "\n",
    "The training error will tend to decrease as we increase the degree ‘d’ of the polynomial. At the same time, the cross validation error will tend to decrease as we increase ‘d’ up to a point, and then it will increase as ‘d’ is increased, forming a convex curve.\n",
    "The is summarized in the figure below:\n",
    "![](../../img/lec05_bias_variance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Typical learning curve for high variance**\n",
    "![](../../img/lec05_high_variance.png)\n",
    "\n",
    "**Typical learning curve for high bias**\n",
    "![](../../img/lec05_high_bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnostic tell you what to try next:**\n",
    "\n",
    "Fixes to try:\n",
    "* Try getting more training examples - fixes high variance\n",
    "* Try a smaller set of features - fixes high variance\n",
    "* Try a larger set of features - fixes high bias.\n",
    "* Try add additional features - fixes high bias.\n",
    "* Run gradient descent for more iterations - fixes optimization algorithm.\n",
    "* Try Newton’s method - fixes optimization algorithm.\n",
    "* Use a different value for λ - fixes optimization objective.\n",
    "* Use higher value for regularization parameter - fix high variance (Lasso or Ridge)\n",
    "* Try using an SVM - fixes optimization objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
